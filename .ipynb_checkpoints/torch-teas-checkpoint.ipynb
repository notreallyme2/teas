{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, tensor, FloatTensor\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "from fastai.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_expression_data = pd.read_csv(\"data/net1_expression_data.tsv\", sep='\\t')\n",
    "# chip_features = pd.read_csv(\"data/net1_chip_features.tsv\", sep='\\t')\n",
    "# gene_ids = pd.read_csv(\"data/net1_gene_ids.tsv\", sep='\\t')\n",
    "tfs = pd.read_csv(\"data/net1_transcription_factors.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_idx = [g in tfs.iloc[:,0].tolist() for g in synthetic_expression_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(805, 600)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_expression_data.values[:,:600].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = map(torch.tensor, (synthetic_expression_data.values[:600, :], synthetic_expression_data.values[600:, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_ds = TensorDataset(train[:, X_idx], train)\n",
    "valid_ds = TensorDataset(valid[:, X_idx], valid)\n",
    "data = DataBunch.create(train_ds, valid_ds, bs = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 194]), torch.Size([128, 1643]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter(data.train_dl))\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = TeaDataset(synthetic_expression_data.values, X_idx, X_in_Y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1643])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_ = next(iter(synthetic_data))\n",
    "next_['Y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FEA(nn.Module):\n",
    "    \"\"\"A pytorch module to build a (standard) Forward-Embedding Autoencoders\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            The number of input features\n",
    "        hidden_dim : int\n",
    "            The number of features in the hidden layer\n",
    "        output_dim : int\n",
    "            The number of output features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#     def train(self, data_loader, optimizer, criterion):\n",
    "#         \"\"\"\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         data_loader : torch.utils.data.DataLoader\n",
    "#         optimizer : torch.optim\n",
    "#         criterion : function\n",
    "#         \"\"\"\n",
    "#         running_loss = 0\n",
    "#         for i, row in enumerate(data_loader):\n",
    "#             optimizer.zero_grad()\n",
    "#             y_hat = self.forward(row['X'])\n",
    "#             loss = criterion(y_hat, row['y'].view(y_hat.size()[0],1))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()    \n",
    "#             running_loss += loss.item()\n",
    "#         print(\"Train loss: {0:.3f}\".format(running_loss / (i + 1))) # loss needs to be averaged over all batches\n",
    "#         return(running_loss / (i + 1))\n",
    "\n",
    "#     def test(self, data_loader, criterion):\n",
    "#         \"\"\"\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         data_loader : torch.utils.data.DataLoader\n",
    "#         criterion : function\n",
    "#         \"\"\"\n",
    "#         running_loss = 0\n",
    "#         for i, row in enumerate(data_loader):\n",
    "#             y_hat = self.forward(row['X'])\n",
    "#             loss = criterion(y_hat, row['y'].view(y_hat.size()[0],1)) \n",
    "#             running_loss += loss.item()\n",
    "#         print(\"Test loss: {0:.3f}\".format(running_loss / (i + 1))) # loss needs to be averaged over all batches\n",
    "#         return(running_loss / (i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_model = FEA(input_dim = sum(X_idx), hidden_dim = 256, output_dim = synthetic_expression_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FEA(\n",
       "  (fc1): Linear(in_features=194, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=1643, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeaDataset(Dataset):\n",
    "    \"\"\"A custom Pytorch Dataset class for training FEA and TEA models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, all_data, X_idx, X_in_Y=False):\n",
    "        \"\"\"\n",
    "        Paramaters\n",
    "        ----------\n",
    "        all_data : np.array[float]\n",
    "            All the data (X and y)x\n",
    "        X_in_Y : bool\n",
    "            Should the predictors be included in the target array (defaults to False)?\n",
    "        X_idx : list[bool]\n",
    "            Indexes of the input variables (columns) in all_data\n",
    "        \"\"\"      \n",
    "        self.all_data = all_data\n",
    "        self.X = all_data[:,X_idx]\n",
    "        if X_in_Y:\n",
    "            self.Y = all_data\n",
    "        else:\n",
    "            not_Y = [not e for e in X_idx]\n",
    "            self.Y = all_data[:,not_Y]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the object\"\"\"\n",
    "        return self.all_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a single sample from the dataset\"\"\"\n",
    "        sample = {'X': tensor(self.X[idx]), 'Y': tensor(self.Y[idx])}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Take a sample from a SyntheticClassificationDataset and convert it to a Pytorch tensor\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        X, y = sample['X'], sample['y']\n",
    "        \n",
    "        transformed_sample = {\n",
    "            'X': tensor(X).type(FloatTensor), \n",
    "            'y': tensor(y).type(FloatTensor)}\n",
    "        return transformed_sample\n",
    "\n",
    "class Normalise(object):\n",
    "    \"\"\"Take a sample from a SyntheticClassificationDataset and normalise the input features (X)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        method : str\n",
    "            From 'z' or 'range'. The default is 'z'.\n",
    "            'z' z-transforms the features\n",
    "            'range' range normalises so that the data lie in the range [0,1]\n",
    "    \"\"\"\n",
    "    def __init__(self, method = 'z'):\n",
    "        assert isinstance(method, str)\n",
    "        assert method in ['z', 'range']\n",
    "        self.method = method\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        X, y = sample['X'], sample['y']\n",
    "        if self.method == \"z\":\n",
    "            normalised_X = (X - X.mean())/ X.std()\n",
    "        else:\n",
    "            normalised_X = (X - X.min())/X.max()\n",
    "\n",
    "        return {'X': normalised_X, 'y': y}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
